---
page_title: "dbtcloud_global_connection Resource - dbtcloud"
subcategory: ""
description: |-
  This resource can be used to create global connections as introduced in dbt Cloud in August 2024.
  Those connections are not linked to a project and can be linked to environments from different projects by using the connection_id field in the dbtcloud_environment resource.
  For now, only a subset of connections are supported and the other Data Warehouses can continue using the existing resources dbtcloud_connection and dbtcloud_fabric_connection ,
  but all Data Warehouses will soon be supported under this resource and the other ones will be deprecated in the future.
---

# dbtcloud_global_connection (Resource)


This resource can be used to create global connections as introduced in dbt Cloud in August 2024.

Those connections are not linked to a project and can be linked to environments from different projects by using the `connection_id` field in the `dbtcloud_environment` resource.

For now, only a subset of connections are supported and the other Data Warehouses can continue using the existing resources `dbtcloud_connection` and `dbtcloud_fabric_connection` , 
but all Data Warehouses will soon be supported under this resource and the other ones will be deprecated in the future.

## Example Usage

```terraform
resource "dbtcloud_global_connection" "bigquery" {
  name = "My BigQuery connection"
  bigquery = {
    gcp_project_id              = "my-gcp-project-id"
    timeout_seconds             = 1000
    private_key_id              = "my-private-key-id"
    private_key                 = "ABCDEFGHIJKL"
    client_email                = "my_client_email"
    client_id                   = "my_client_id"
    auth_uri                    = "my_auth_uri"
    token_uri                   = "my_token_uri"
    auth_provider_x509_cert_url = "my_auth_provider_x509_cert_url"
    client_x509_cert_url        = "my_client_x509_cert_url"
    application_id              = "oauth_application_id"
    application_secret          = "oauth_secret_id"
  }
}

resource "dbtcloud_global_connection" "databricks" {
  name = "My Databricks connection"
  databricks = {
    host = "my-databricks-host.cloud.databricks.com"
    http_path = "/sql/my/http/path"

    // optional fields
    catalog = "dbt_catalog"
    client_id = "yourclientid"
    client_secret = "yourclientsecret"
  }
}

resource "dbtcloud_global_connection" "snowflake" {
  name = "My Snowflake connection"
  // we can set Privatelink if needed
  private_link_endpoint_id = data.dbtcloud_privatelink_endpoint.my_private_link.id
  snowflake = {
    account                   = "my-snowflake-account"
    database                  = "MY_DATABASE"
    warehouse                 = "MY_WAREHOUSE"
    client_session_keep_alive = false
    allow_sso                 = true
    oauth_client_id           = "yourclientid"
    oauth_client_secret       = "yourclientsecret"
  }
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `name` (String) Connection name

### Optional

- `bigquery` (Attributes) (see [below for nested schema](#nestedatt--bigquery))
- `databricks` (Attributes) Databricks connection configuration (see [below for nested schema](#nestedatt--databricks))
- `private_link_endpoint_id` (String) Private Link Endpoint ID. This ID can be found using the `privatelink_endpoint` data source
- `snowflake` (Attributes) Snowflake connection configuration (see [below for nested schema](#nestedatt--snowflake))

### Read-Only

- `adapter_version` (String) Version of the adapter
- `id` (Number) Connection Identifier
- `is_ssh_tunnel_enabled` (Boolean) Whether the connection can use an SSH tunnel
- `oauth_configuration_id` (Number)

<a id="nestedatt--bigquery"></a>
### Nested Schema for `bigquery`

Required:

- `auth_provider_x509_cert_url` (String) Auth Provider X509 Cert URL for the Service Account
- `auth_uri` (String) Auth URI for the Service Account
- `client_email` (String) Service Account email
- `client_id` (String) Client ID of the Service Account
- `client_x509_cert_url` (String) Client X509 Cert URL for the Service Account
- `gcp_project_id` (String) The GCP project ID to use for the connection
- `private_key` (String, Sensitive) Private Key for the Service Account
- `private_key_id` (String) Private Key ID for the Service Account
- `token_uri` (String) Token URI for the Service Account

Optional:

- `application_id` (String, Sensitive) OAuth Client ID
- `application_secret` (String, Sensitive) OAuth Client Secret
- `dataproc_cluster_name` (String) Dataproc cluster name for PySpark workloads
- `dataproc_region` (String) Google Cloud region for PySpark workloads on Dataproc
- `execution_project` (String) Project to bill for query execution
- `gcs_bucket` (String) URI for a Google Cloud Storage bucket to host Python code executed via Datapro
- `impersonate_service_account` (String) Service Account to impersonate when running queries
- `job_creation_timeout_seconds` (Number) Maximum timeout for the job creation step
- `job_retry_deadline_seconds` (Number) Total number of seconds to wait while retrying the same query
- `location` (String) Location to create new Datasets in
- `maximum_bytes_billed` (Number) Max number of bytes that can be billed for a given BigQuery query
- `priority` (String) The priority with which to execute BigQuery queries (batch or interactive)
- `retries` (Number) Number of retries for queries
- `scopes` (Set of String) OAuth scopes for the BigQuery connection
- `timeout_seconds` (Number) Timeout in seconds for queries


<a id="nestedatt--databricks"></a>
### Nested Schema for `databricks`

Required:

- `host` (String) The hostname of the Databricks cluster or SQL warehouse.
- `http_path` (String) The HTTP path of the Databricks cluster or SQL warehouse.

Optional:

- `catalog` (String) Catalog name if Unity Catalog is enabled in your Databricks workspace.
- `client_id` (String) Required to enable Databricks OAuth authentication for IDE developers.
- `client_secret` (String) Required to enable Databricks OAuth authentication for IDE developers.


<a id="nestedatt--snowflake"></a>
### Nested Schema for `snowflake`

Required:

- `account` (String) The Snowflake account name
- `database` (String) The default database for the connection
- `warehouse` (String) The default Snowflake Warehouse to use for the connection

Optional:

- `allow_sso` (Boolean) Whether to allow Snowflake OAuth for the connection. If true, the `oauth_client_id` and `oauth_client_secret` fields must be set
- `client_session_keep_alive` (Boolean) If true, the snowflake client will keep connections for longer than the default 4 hours. This is helpful when particularly long-running queries are executing (> 4 hours)
- `oauth_client_id` (String, Sensitive) OAuth Client ID. Required to allow OAuth between dbt Cloud and Snowflake
- `oauth_client_secret` (String, Sensitive) OAuth Client Secret. Required to allow OAuth between dbt Cloud and Snowflake
- `role` (String) The Snowflake role to use when running queries on the connection

## Import

Import is supported using the following syntax:

```shell
# A project-scoped connection can be imported as a global connection by specifying the connection ID
# Migrating from project-scoped connections to global connections could be done by:
# 1. Adding the config for the global connection and importing it (see below)
# 2. Removing the project-scoped connection from the config AND from the state
#    - CAREFUL: If the connection is removed from the config but not the state, it will be destroyed on the next apply


# using  import blocks (requires Terraform >= 1.5)
import {
  to = dbtcloud_global_connection.my_connection
  id = "connection_id"
}

import {
  to = dbtcloud_global_connection.my_connection
  id = "1234"
}

# using the older import command
terraform import dbtcloud_global_connection.my_connection "connection_id"
terraform import dbtcloud_global_connection.my_connection 1234
```
